{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3DV-6oUOLlh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer,StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data_csv/')\n",
    "\n",
    "data = {}\n",
    "for file in data_dir.iterdir():\n",
    "    name = file.stem.split('_')[1]\n",
    "    data[name]  = pd.read_csv(file.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['TrackP', 'TrackEta', 'NumLongTracks']\n",
    "y_cols = ['RichDLLbt', 'RichDLLk', 'RichDLLmu', 'RichDLLp', 'RichDLLe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgD-M-C7rsBs"
   },
   "source": [
    "One can see there's a peak of outliers at the left handside of the plot. Let's not bother about it so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(X, Y, Y_pred, n_slices=100):\n",
    "    score = 0\n",
    "    \n",
    "    \n",
    "    reference = pd.concat([X, Y], axis=1).values\n",
    "    prediction = pd.concat([X,Y_pred], axis=1).values\n",
    "    \n",
    "    w_normal = np.random.normal(size=(n_slices, reference.shape[1]))\n",
    "    \n",
    "    \n",
    "    for k in range(n_slices):\n",
    "        score = max(score,\n",
    "                    ks_2samp(\n",
    "                        np.sum(w_normal[k] * reference, axis=1),\n",
    "                        np.sum(w_normal[k] * prediction, axis=1)\n",
    "                    )[0]\n",
    "                   )\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def cross_val(X, Y, Model, **kwargs):\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    model = Model(**kwargs)\n",
    "    model_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,Y):\n",
    "        X_train = X.iloc[train_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        X_test  = X.iloc[test_index ]\n",
    "        Y_test  = Y.iloc[test_index ]\n",
    "  \n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "  \n",
    "        model_scores.append(score_func(X_test, Y_test, Y_pred))\n",
    "    \n",
    "    return model_scores\n",
    "\n",
    "def compare_hists(Y_test, Y_pred, name):\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
    "    i = 0\n",
    "    for col in Y_pred.columns:\n",
    "        _, bins, _ = axes[i].hist(Y_test[col], bins=100 , label='test'      )\n",
    "        _, _   , _ = axes[i].hist(Y_pred[col], bins=bins, label='prediction', alpha=0.7)\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel(\"%s: %s\" % (name, col))\n",
    "        i += 1\n",
    "    fig.show();\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conditional Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y|X)\\sim N(\\mu(X),\\Sigma(X))$\n",
    "\n",
    "First we go for model with $Cov(y_i,y_j)=0, i\\not =j$, namely distinct model for each component of $Y$. Also for simplicity we condition on only one component of $X$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pion'][x_cols]\n",
    "Y = data['pion'][y_cols]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Независимо бьем каждый из предикторов по квантилям. Для каждого \"бина\" считаем условное среднее/медиану и стандартное отклонение. model_outliers=True пытается учитывать экзит коды: Смотрим где все таргеты нулевые или -999, считаем долю таких событий, при генерации сначала сэмплим бернули на наличие выброса, потом сэмплим или нормально или снова бернули на тип выброса (0 или -999). Также не используем их при вычислении бинов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1:\n",
    "    def __init__(self, n_bins=7, model_outliers=False):\n",
    "        self.n_bins = n_bins\n",
    "        self.model_outliers = model_outliers\n",
    " \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        X = X.copy()\n",
    "        Y = Y.copy()\n",
    "        \n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean()\n",
    "        \n",
    "        self.binarizer = KBinsDiscretizer(n_bins=self.n_bins, encode='onehot-dense')\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.ohe_cols = ['{}_{}'.format(a, b+1) \n",
    "                for a,b \n",
    "                in zip(np.repeat(self.x_cols,self.n_bins), np.tile(range(self.n_bins), len(self.x_cols)))\n",
    "                ] \n",
    "        \n",
    "        labels = self.binarizer.fit_transform(X)\n",
    "        labels = pd.DataFrame(labels, columns = self.ohe_cols, dtype=np.int8,index=X.index)\n",
    "        \n",
    "        train = pd.concat([labels, Y], axis=1)\n",
    "        \n",
    "        self.gmean = Y.mean()\n",
    "        self.gstd = Y.std()\n",
    "        \n",
    "        self.means = train.groupby(self.ohe_cols,as_index=False).mean()\n",
    "        self.stds = train.groupby(self.ohe_cols, as_index=False).std(ddof=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        labels = self.binarizer.transform(X)\n",
    "        labels = pd.DataFrame(labels, columns=self.ohe_cols, dtype=np.int8, index=X.index)\n",
    "        \n",
    "        means = pd.merge(labels, self.means, how='left')[self.y_cols].fillna(self.gmean)\n",
    "        stds = pd.merge(labels, self.stds, how='left')[self.y_cols].fillna(self.gstd)\n",
    "        \n",
    "\n",
    "        pred = np.random.normal(loc=means, scale=stds)\n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        \n",
    "        return pd.DataFrame(pred, columns = self.y_cols, index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь тоже самое, но для каждого бина оцениваем характерное значение предиктров(среднее/медиана по бину) и строим линейную регрессию на на среднее и стандартное отклонение таргетов. На предикте уже не смотрим в какой бин попал пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2:\n",
    "    def __init__(self, n_bins=7, model_outliers=False):\n",
    "        self.n_bins = n_bins\n",
    "        self.model_outliers= model_outliers\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "    \n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "\n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean()\n",
    "        \n",
    "        self.binarizer = KBinsDiscretizer(n_bins=self.n_bins, encode='onehot-dense')\n",
    "        self.ohe_cols = ['{}_{}'.format(a, b+1) \n",
    "                for a,b \n",
    "                in zip(np.repeat(self.x_cols,self.n_bins), np.tile(range(self.n_bins), len(self.x_cols)))\n",
    "                ] \n",
    "        \n",
    "        labels = self.binarizer.fit_transform(X)\n",
    "        labels = pd.DataFrame(labels, columns = self.ohe_cols, dtype=np.int8, index=X.index)\n",
    "        \n",
    "        train = pd.concat([labels,X, Y], axis=1)\n",
    "        \n",
    "        y_means = train.groupby(self.ohe_cols, as_index=False).median()[self.y_cols].fillna(Y.median())\n",
    "        y_stds = train.groupby(self.ohe_cols, as_index=False).std(ddof=0)[self.y_cols].fillna(Y.std())\n",
    "        self.x_medians = train.groupby(self.ohe_cols, as_index=False)[self.x_cols].median().fillna(X.median())\n",
    "        \n",
    "\n",
    "        self.mean_regr = LinearRegression(normalize=True, n_jobs=-1)\n",
    "        self.std_regr = LinearRegression(normalize=True, n_jobs=-1)\n",
    "        \n",
    "        self.mean_regr.fit(self.x_medians[self.x_cols], y_means)\n",
    "        self.std_regr.fit(self.x_medians[self.x_cols], y_stds)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \n",
    "        means = self.mean_regr.predict(X)\n",
    "        stds = self.std_regr.predict(X)\n",
    "\n",
    "        pred = np.random.normal(loc=means, scale=np.maximum(0,stds))\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        \n",
    "        \n",
    "        \n",
    "        return pd.DataFrame(pred, columns = self.y_cols, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030775753793969685"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_1(15)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X_test,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10983912128702966"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_1(15,True)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X_test,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4049607603139175"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_2(15)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X_test,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08030735754113966"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_2(15, True)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X_test,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще из-за того что разбиваем на бины независимо по предикторам, то мы можем получать бины  из 1-2 элементов. \n",
    "Есть идея кластеризовать с помощью k-means в 3х мерном пространстве $X$ и использовать кластеры как бины. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3():\n",
    "    def __init__(self, n_bins=7, model_outliers=False):\n",
    "        self.n_bins = n_bins\n",
    "        self.model_outliers= model_outliers\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.y_cols = Y.columns\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean() \n",
    "            \n",
    "        kmeans = KMeans(n_clusters=self.n_bins, n_jobs=-1)\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        train = Y.copy()\n",
    "        train['labels']  = labels\n",
    "        means = train.groupby('labels').mean()\n",
    "        stds = train.groupby('labels').std()\n",
    "        \n",
    "        self.kmeans = kmeans\n",
    "        self.scaler = scaler\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        labels = self.kmeans.predict(X_scaled)\n",
    "        \n",
    "        means = self.means.iloc[labels]\n",
    "        stds = self.stds.iloc[labels]\n",
    "        \n",
    "        pred = np.random.normal(loc=means, scale=stds)\n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        return pd.DataFrame(pred, columns = self.y_cols,index=X.index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0862433100535196"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model_3(n_bins=50,model_outliers=True)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "\n",
    "score_func(X_test, Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for n_bins in [5,10,15,20,25]:\n",
    "    scores[n_bins] = np.mean(cross_val(X,Y,Model_1, n_bins=n_bins, model_outliers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.03374179307141685,\n",
       " 10: 0.03610774285705175,\n",
       " 15: 0.030001823911141246,\n",
       " 20: 0.04944872500665249,\n",
       " 25: 0.031101817246186214}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.06803873698631509,\n",
       " 10: 0.08455765998170012,\n",
       " 15: 0.0832966786216069,\n",
       " 20: 0.10715258822705889,\n",
       " 25: 0.09667968625156874}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "for n_bins in [5,10,15,20,25]:\n",
    "    scores[n_bins] = np.mean(cross_val(X,Y,Model_2, n_bins=n_bins, model_outliers=True))\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for p in data:\n",
    "    X = data[p][x_cols]\n",
    "    Y = data[p][y_cols]\n",
    "    model_scores[p] = cross_val(X,Y, Model_1, n_bins=15, model_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muon:\t 0.04945997345013275\n",
      "ghost:\t 0.055964279821399135\n",
      "kaon:\t 0.011767075330541613\n",
      "proton:\t 0.01032406151040649\n",
      "pion:\t 0.02247784896114171\n",
      "electron:\t 0.03180486823065887\n"
     ]
    }
   ],
   "source": [
    "for p in model_scores:\n",
    "    print(f'{p}:\\t',np.mean(model_scores[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Старая реализация без KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(x, bounds):\n",
    "    lefts = bounds[:-1]\n",
    "    lefts[0] -= 0.1\n",
    "    rights = bounds[1:]\n",
    "    \n",
    "    a = np.repeat(x, len(rights)).reshape((-1,len(rights))) <= rights\n",
    "    b = np.repeat(x, len(lefts)).reshape((-1,len(lefts))) > lefts\n",
    "    \n",
    "    return np.where(a & b)[1]\n",
    "\n",
    "def f(x, y_cols):\n",
    "    d = {}\n",
    "    for col in y_cols:\n",
    "        d[f'{col}_mean'] = x[col].mean()\n",
    "        d[f'{col}_std'] =  x[col].std()\n",
    "\n",
    "    return pd.Series(d,list(d.keys()))\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self,n_intervals=4):\n",
    "        self.n_intervals = n_intervals\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "        \n",
    "        data = pd.concat([X.copy(), Y.copy()], axis=1)\n",
    "        masks = pd.DataFrame(index=X.index, columns=X.columns)\n",
    "        \n",
    "        quants = [i/self.n_intervals for i in range(self.n_intervals + 1)]\n",
    "        bounds =  np.quantile(X, quants, axis=0)\n",
    "        \n",
    "        for i,col in enumerate(X):\n",
    "            masks[col] = make_mask(X[col].values, bounds[:,i]).astype(str)\n",
    "        \n",
    "        \n",
    "        data['mask'] = masks.sum(axis=1)\n",
    "        \n",
    "        train = data.groupby('mask').apply(lambda x: f(x, self.y_cols))\n",
    "        n_bins = pd.unique(data['mask']).shape[0]\n",
    "        assert n_bins == self.n_intervals**3\n",
    "        \n",
    "        self.ohe = OneHotEncoder(sparse= False)\n",
    "        self.ohe.fit_transform(data['mask'])\n",
    "        \n",
    "        self.means = LinearRegression()\n",
    "        self.stds = LinearRegression()\n",
    "        \n",
    "        \n",
    "        self.means.fit(train[self.x_cols].values, train[[f'{col}_mean' for col in self.y_cols]].values)\n",
    "        self.stds.fit(train[self.x_cols].values, train[[f'{col}_std' for col in self.y_cols]].values)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prediction = pd.DataFrame()\n",
    "        means = self.means.predict(X.values)\n",
    "        stds = self.stds.predict(X.values)\n",
    "        pred = np.random.standard_normal()*stds + means\n",
    "        return pd.DataFrame(pred, columns = self.y_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(3)\n",
    "m.train(X,Y)\n",
    "y = m.predict(X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RICH_Data_Overview.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
