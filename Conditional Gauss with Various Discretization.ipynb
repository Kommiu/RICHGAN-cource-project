{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3DV-6oUOLlh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer,StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./data_csv/')\n",
    "\n",
    "data = {}\n",
    "for file in data_dir.iterdir():\n",
    "    name = file.stem.split('_')[1]\n",
    "    data[name]  = pd.read_csv(file.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['TrackP', 'TrackEta', 'NumLongTracks']\n",
    "y_cols = ['RichDLLbt', 'RichDLLk', 'RichDLLmu', 'RichDLLp', 'RichDLLe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgD-M-C7rsBs"
   },
   "source": [
    "One can see there's a peak of outliers at the left handside of the plot. Let's not bother about it so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(X, Y, Y_pred, n_slices=100):\n",
    "    score = 0\n",
    "    \n",
    "    \n",
    "    reference = pd.concat([X, Y], axis=1).values\n",
    "    w_normal = np.random.normal(size=(n_slices, reference.shape[1]))\n",
    "    prediction = pd.concat([X,Y_pred], axis=1).values\n",
    "    \n",
    "    for k in range(n_slices):\n",
    "        score = max(score,\n",
    "                    ks_2samp(\n",
    "                        np.sum(w_normal[k] * reference, axis=1),\n",
    "                        np.sum(w_normal[k] * prediction, axis=1)\n",
    "                    )[0]\n",
    "                   )\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def cross_val(X, Y, model):\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    model_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,Y):\n",
    "        X_train = X.iloc[train_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        X_test  = X.iloc[test_index ]\n",
    "        Y_test  = Y.iloc[test_index ]\n",
    "  \n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "  \n",
    "        model_scores.append(score_func(X_test, Y_test, Y_pred))\n",
    "    \n",
    "    return model_scores\n",
    "\n",
    "def compare_hists(Y_test, Y_pred, name):\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(8, 20))\n",
    "    i = 0\n",
    "    for col in Y_pred.columns:\n",
    "        _, bins, _ = axes[i].hist(Y_test[col], bins=100 , label='test'      )\n",
    "        _, _   , _ = axes[i].hist(Y_pred[col], bins=bins, label='prediction', alpha=0.7)\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel(\"%s: %s\" % (name, col))\n",
    "        i += 1\n",
    "    fig.show();\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conditional Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y|X)\\sim N(\\mu(X),\\Sigma(X))$\n",
    "\n",
    "First we go for model with $Cov(y_i,y_j)=0, i\\not =j$, namely distinct model for each component of $Y$. Also for simplicity we condition on only one component of $X$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pion'][x_cols]\n",
    "Y = data['pion'][y_cols]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Независимо бьем каждый из предикторов по квантилям. Для каждого \"бина\" считаем условное среднее/медиану и стандартное отклонение. model_outliers=True пытается учитывать экзит коды: Смотрим где все таргеты нулевые или -999, считаем долю таких событий, при генерации сначала сэмплим бернули на наличие выброса, потом сэмплим или нормально или снова бернули на тип выброса (0 или -999). Также не используем их при вычислении бинов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1:\n",
    "    def __init__(self, n_bins=7, model_outliers=False):\n",
    "        self.n_bins = n_bins\n",
    "        self.model_outliers = model_outliers\n",
    " \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        X = X.copy()\n",
    "        Y = Y.copy()\n",
    "        \n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean()\n",
    "        \n",
    "        self.binarizer = KBinsDiscretizer(n_bins=self.n_bins, encode='onehot-dense')\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.ohe_cols = ['{}_{}'.format(a, b+1) \n",
    "                for a,b \n",
    "                in zip(np.repeat(self.x_cols,self.n_bins), np.tile(range(self.n_bins), len(self.x_cols)))\n",
    "                ] \n",
    "        \n",
    "        labels = self.binarizer.fit_transform(X)\n",
    "        labels = pd.DataFrame(labels, columns = self.ohe_cols, dtype=np.int8)\n",
    "        \n",
    "        train = pd.concat([labels, Y], axis=1)\n",
    "        \n",
    "        self.means = train.groupby(self.ohe_cols).mean().reset_index().fillna(Y.mean())\n",
    "        self.stds = train.groupby(self.ohe_cols).std(ddof=0).reset_index().fillna(Y.std())\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        labels = self.binarizer.transform(X)\n",
    "        labels = pd.DataFrame(labels, columns=self.ohe_cols, dtype=np.int8)\n",
    "        \n",
    "        means = pd.merge(labels, self.means, how='left')[self.y_cols]\n",
    "        stds = pd.merge(labels, self.stds, how='left')[self.y_cols]\n",
    "        \n",
    "\n",
    "        pred = np.random.normal(loc=means, scale=stds)\n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        \n",
    "        return pd.DataFrame(pred, columns = self.y_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь тоже самое, но для каждого бина оцениваем характерное значение предиктров(среднее/медиана по бину) и строим линейную регрессию на на среднее и стандартное отклонение таргетов. На предикте уже не смотрим в какой бин попал пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2:\n",
    "    def __init__(self, n_bins=7, model_outliers=False):\n",
    "        self.n_bins = n_bins\n",
    "        self.model_outliers= model_outliers\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "    \n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "\n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean()\n",
    "        \n",
    "        self.binarizer = KBinsDiscretizer(n_bins=self.n_bins, encode='onehot-dense')\n",
    "        self.ohe_cols = ['{}_{}'.format(a, b+1) \n",
    "                for a,b \n",
    "                in zip(np.repeat(self.x_cols,self.n_bins), np.tile(range(self.n_bins), len(self.x_cols)))\n",
    "                ] \n",
    "        \n",
    "        labels = self.binarizer.fit_transform(X)\n",
    "        labels = pd.DataFrame(labels, columns = self.ohe_cols, dtype=np.int8)\n",
    "        \n",
    "        train = pd.concat([labels,X, Y], axis=1)\n",
    "        \n",
    "        y_means = train.groupby(self.ohe_cols).median().reset_index()[self.y_cols].fillna(Y.median())\n",
    "        y_stds = train.groupby(self.ohe_cols).std(ddof=0).reset_index()[self.y_cols].fillna(Y.std())\n",
    "        self.x_medians = train.groupby(self.ohe_cols)[self.x_cols].median().reset_index().fillna(X.median())\n",
    "        \n",
    "\n",
    "        self.mean_regr = LinearRegression(normalize=True, n_jobs=-1)\n",
    "        self.std_regr = LinearRegression(normalize=True, n_jobs=-1)\n",
    "        \n",
    "        self.mean_regr.fit(self.x_medians[self.x_cols], y_means)\n",
    "        self.std_regr.fit(self.x_medians[self.x_cols], y_stds)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \n",
    "        means = self.mean_regr.predict(X)\n",
    "        stds = self.std_regr.predict(X)\n",
    "\n",
    "        pred = np.random.normal(loc=means, scale=np.maximum(0,stds))\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        \n",
    "        \n",
    "        \n",
    "        return pd.DataFrame(pred, columns = self.y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08704247774513353"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_1(15)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011182932902402604"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_1(15,True)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08351149893100643"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_2(15)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02304486173082962"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m  = Model_2(15, True)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n",
    "score_func(X,Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще из-за того что разбиваем на бины независимо по предикторам, то мы можем получать бины  из 1-2 элементов. \n",
    "Есть идея кластеризовать с помощью k-means в 3х мерном пространстве $X$ и использовать кластеры как бины. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(Model_1):\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.y_cols = Y.columns\n",
    "        \n",
    "        if self.model_outliers:\n",
    "            mask1 = (Y == -999).values.all(axis=1)\n",
    "            mask2 = (Y == 0).values.all(axis=1)\n",
    "            mask = (mask1 | mask2)\n",
    "        \n",
    "            Y = Y[~mask]\n",
    "            X = X[~mask] \n",
    "\n",
    "            self.probs1 = mask.mean()\n",
    "            self.probs2 = mask1.mean()/mask.mean()        \n",
    "        kmeans = KMeans(n_clusters=self.n_bins, n_jobs=-1)\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        train = Y.copy()\n",
    "        train['labels']  = labels\n",
    "        means = train.groupby('labels').mean()\n",
    "        stds = train.groupby('labels').std()\n",
    "        \n",
    "        self.kmeans = kmeans\n",
    "        self.scaler = scaler\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        labels = self.kmeans.predict(X_scaled)\n",
    "        \n",
    "        means = self.means.iloc[labels]\n",
    "        stds = self.stds.iloc[labels]\n",
    "        \n",
    "        pred = np.random.normal(loc=means, scale=stds)\n",
    "        if self.model_outliers:\n",
    "            step1 = np.random.binomial(1, self.probs1,(len(X),1))\n",
    "            step2 = np.random.binomial(1, self.probs2, (len(X),1))\n",
    "            pred = -999*step1*step2 + (1-step1)*pred\n",
    "        return pd.DataFrame(pred, columns = self.y_cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score_func() missing 1 required positional argument: 'Y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d9ef1833988a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: score_func() missing 1 required positional argument: 'Y_pred'"
     ]
    }
   ],
   "source": [
    "m = Model_3(n_bins=100)\n",
    "m.fit(X_train, Y_train)\n",
    "Y_pred = m.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032987802073187564"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func(X, Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for p in data:\n",
    "    X = data[p][x_cols]\n",
    "    Y = data[p][y_cols]\n",
    "    model_scores[p] = cross_val(X,Y, Model_1(15, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Старая реализация без KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09280453597732014, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.05312526562632813, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.063960319801599, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.045925229626148124, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.09484405155948442, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.11908940455297723, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for p in model_scores:\n",
    "    print(model_scores[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(x, bounds):\n",
    "    lefts = bounds[:-1]\n",
    "    lefts[0] -= 0.1\n",
    "    rights = bounds[1:]\n",
    "    \n",
    "    a = np.repeat(x, len(rights)).reshape((-1,len(rights))) <= rights\n",
    "    b = np.repeat(x, len(lefts)).reshape((-1,len(lefts))) > lefts\n",
    "    \n",
    "    return np.where(a & b)[1]\n",
    "\n",
    "def f(x, y_cols):\n",
    "    d = {}\n",
    "    for col in y_cols:\n",
    "        d[f'{col}_mean'] = x[col].mean()\n",
    "        d[f'{col}_std'] =  x[col].std()\n",
    "\n",
    "    return pd.Series(d,list(d.keys()))\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self,n_intervals=4):\n",
    "        self.n_intervals = n_intervals\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        self.x_cols = X.columns.tolist()\n",
    "        self.y_cols = Y.columns.tolist()\n",
    "        \n",
    "        data = pd.concat([X.copy(), Y.copy()], axis=1)\n",
    "        masks = pd.DataFrame(index=X.index, columns=X.columns)\n",
    "        \n",
    "        quants = [i/self.n_intervals for i in range(self.n_intervals + 1)]\n",
    "        bounds =  np.quantile(X, quants, axis=0)\n",
    "        \n",
    "        for i,col in enumerate(X):\n",
    "            masks[col] = make_mask(X[col].values, bounds[:,i]).astype(str)\n",
    "        \n",
    "        \n",
    "        data['mask'] = masks.sum(axis=1)\n",
    "        \n",
    "        train = data.groupby('mask').apply(lambda x: f(x, self.y_cols))\n",
    "        n_bins = pd.unique(data['mask']).shape[0]\n",
    "        assert n_bins == self.n_intervals**3\n",
    "        \n",
    "        self.ohe = OneHotEncoder(sparse= False)\n",
    "        self.ohe.fit_transform(data['mask'])\n",
    "        \n",
    "        self.means = LinearRegression()\n",
    "        self.stds = LinearRegression()\n",
    "        \n",
    "        \n",
    "        self.means.fit(train[self.x_cols].values, train[[f'{col}_mean' for col in self.y_cols]].values)\n",
    "        self.stds.fit(train[self.x_cols].values, train[[f'{col}_std' for col in self.y_cols]].values)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prediction = pd.DataFrame()\n",
    "        means = self.means.predict(X.values)\n",
    "        stds = self.stds.predict(X.values)\n",
    "        pred = np.random.standard_normal()*stds + means\n",
    "        return pd.DataFrame(pred, columns = self.y_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(3)\n",
    "m.train(X,Y)\n",
    "y = m.predict(X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RICH_Data_Overview.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
